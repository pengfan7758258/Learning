通过强化学习激励 LLMs 的推理能力

# 目的
强调RL在大模型中起到重要的推理作用

# 原始论文
![[Deepseek-R1.pdf#height=400]]

# 两个模型
- Deepseek-R1-Zero：仅通过RL训练模型，无需SFT
	- 验证了仅通过RL（GRPO）训练后的模型展现出强大的推理能力
		- 奖励模型：准确率奖励和格式奖励
			- 准确率奖励：准确性奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定格式（例如，在方框内）提供最终答案，从而实现可靠的基于规则的正确性验证。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈
			- 格式奖励：强制模型将其思考过程放在·`‘<think>’` 和 `‘</think>’`标签之间
		- 随着训练的进行。出现了有意思的"**aha momnet**"![[2025-01-22-Deepseek-R1 4.png]]
	- 问题：
		- 可读性差![[2025-01-22-Deepseek-R1.png]]
		- 语言混合![[2025-01-22-Deepseek-R1 1.png]]
- Deepseek-R1: 与Deepseek-R1-zero一样进行RL训练之前结合了冷启动数据和多阶段学习
	- 能力对标openai-o1-1217
	- 训练步骤：
		1. 收集数千条冷启动数据来微调 DeepSeek-V3-Base 模型
			- 长COT数据集微调模型
				- 数据来源：使用DeepSeek-R1-Zero模型，长COT作为prompt，让模型生成具有反思和验证的详细答案，后续人工修改格式和优化
				- 定义了output format：`|special_token|<reasoning_process>|special_token|<summary>`，其中reasoning_process是COT，summarize用于总结推理结果
		2. 执行 DeepSeek-R1-Zero 的RL学习直至收敛 - 这一阶段关注提升模型的推理能力
			- 还加入了语言一致性reward，该奖励计算为 CoT 中目标语言单词的比例，在消融实验中虽然表明会降低一些性能，但使结果更具有可读性
			- 准确性reward与语言一致性reward相加来形成最终奖励（好像是没有了格式奖励）
		3. 通过2的RL checkpoint进行拒绝采样创建新的 SFT 数据（600k+）并结合来自 DeepSeek-V3 在写作、事实问答和自我认知等领域的监督数据（200k+）共800k+
			- 推理数据（600k+）：
				- 每个prompt生成多个响应，仅保留正确答案的响应，也可以说是得分最高的（generate model reward）
				- 首先这些数据能通过规则奖励，数学的正确答案或者说是代码的测试用例
				- 过滤了混合语言、冗长段落、冗长代码块
				- 通过将真实值和模型预测输入到 DeepSeek-V3 进行判断
			- 非推理数据：写作、事实问答、自我认知和翻译（200k+）
				- 直接复用 DeepSeek-V3的SFT部分数据集
				- 通过prompt调用 DeepSeek-V3 在回答问题之前生成潜在的思考链
				- 对于像“你好”这样的简单查询，我们不提供 CoT 作为回应
		4. 通过3的数据重新训练 DeepSeek-V3-Base 模型
		5. 通过4微调后的模型经过额外的RL过程得到最终的Deepseek-R1模型
- 模型性能对比图：![[2025-01-22-Deepseek-R1 2.png]]


# tempalte
## deepseek-r1-zero
![[2025-01-22-Deepseek-R1 3.png]]

# test-time scaling
![[b3f045cb7ec2974b0605aab3ea8d7a6.png]]

# Rejection Sampling
拒绝采样：让模型生成数据，后面根据算法或人工筛选高质量数据
通过“生成-筛选”机制，从易采样的分布中提取符合目标分布的样本


# 评估数据集
## AIME 2024
**美国数学邀请赛**
- **pass@1**：模型仅生成**一个答案**（不进行多次采样或投票），若该答案与标准答案匹配，则计为通过。
- **cons@k**：k 次采样中多数投票决定的正确率，衡量模型稳定性（如 cons@64）
## MMLU
大规模多任务语言理解基准测试，涵盖 **57 个学科**，包括 STEM、人文、社会科学等
- **问题类型**：4 选项单选题。
## MMLU-Pro（MMLU 专业增强版）
MMLU-Pro 是 MMLU 的升级版，由 **滑铁卢大学、多伦多大学、卡内基梅隆大学** 联合发布
- **问题类型**：**10 选项单选题**（大幅降低随机猜测概率）
## GPQA Diamond（Graduate-Level Google-Proof Q&A Benchmark）
GPQA 是一个 **极端困难** 的问答基准，由 **生物学、物理学、化学领域的博士专家** 编写，旨在测试模型的高阶推理能力
- **问题类型**：448 道 **高难度多选题**。
## SimpleQA
 OpenAI 于 2024 年 10 月推出的一个开源基准测试数据集，专门用于评估语言模型在**事实准确性（Factual Accuracy）**方面的表现，旨在减少 AI 生成错误或未经证实的答案（即“幻觉”现象）
 - 每个问题**仅有一个明确、可验证的答案**，避免模糊性（如“iPhone 哪一年发布？”的答案只能是“2007 年”）
 - 所有问题的参考答案均经过**两名独立 AI 训练师验证**，并附有**可追溯的来源链接**（如官方资料、权威网站），例如，对于“苹果公司创始人是谁？”，训练师会引用苹果官网或历史档案作为证据
## AlpacaEval、ArenaHard
都是使用GPT-4或者更厉害的模型先进模型来作为裁决者来判断目标模型输出的质量和好坏

# GRPO
Group Relative Policy Optimization (群组相对策略优化)
![[2025-01-22-Deepseek-R1 5.png]]


# 模型蒸馏 distill
使用deepseek中微调的80k+数据集在小模型（1.5B、7B、8B、14B、32B、70B）上进行微调得到的模型推理能力会有很大提升


# 总结图
![[deepseek.png]]
![[2025-01-22-Deepseek-R1 6.png]]