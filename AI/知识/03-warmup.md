- 通常在刚开始训练模型时给一个很小的学习率，以避免模型在开始的时候梯度更新过大，导致优化方向过于激烈
- 学习率的 warm-up 通过在训练初期逐渐增大学习率，避免了训练过程中的不稳定现象，尤其是在深度神经网络训练中尤为重要。通过 warm-up，模型可以稳定地开始训练，然后在后期逐步加速收敛，从而得到更好的训练效果。

策略：
- 线性warmup：学习率在指定的步数随着epoch线性增加
- 指数warmup：学习率根据规则指数规则增大

作用：
- **稳定训练过程**：在训练初期通过逐渐增加学习率，模型的参数更新不会过于剧烈，从而避免训练过程中的不稳定。
- **帮助收敛到更好的解**：过高的初始学习率可能会导致在初期就跳过较优的解，而通过 warm-up，模型可以先找到一个较为平滑的优化路径，然后再快速逼近最优解。
- **适应更大的学习率**：当模型在初期已经逐步学习到合理的特征后，warm-up 结束后，学习率可以迅速升高，从而加速后期的收敛

