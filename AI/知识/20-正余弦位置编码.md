transformer最开启提出使用的位置编码方式
![[20-正余弦位置编码-1.png]]

1. token embedding+position encoding=X, 这个会经过W_Q,W_K,W_V的线性变换，但这些线性变换只是做了原始矩阵旋转和拉伸(这里我记得一些矩阵的知识，我也不会啊)，也就是说经过W_Q,W_K,W_V后位置信息还在，只不过更加抽象，然后最后还是作用到了attention上做相关度的计算
2. 假设忽略W_Q,W_K,W_V那就是token embedding+position encoding在做相关度计算
3. token1的vector是[t1+p1]
4. token2的vector是[t2+p2]
5. 两个向量点积: t1t2+t1p2+p1t2+p1p2
6. 主要是p1p2计算出来就是cos(δpos·θ)的位置差