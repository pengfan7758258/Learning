- 可以将企业知识和私有化的知识（本地的pdf、txt等数据）与大模型结合在一起来解决用户的某种需求
- 拿到一个文本在向量化的库中找到和这个文本相关的其它数据
- RAG流程：大量的文档->文档切分（分成若干个段落chunks）->向量化（Embedding）->向量数据库（milvus）
![[00-RAG增强检索是如何工作的？.png]]

## 常见问题
1. 文档类型处理：PDF、PPT、Excel、Csv等
2. 文档切分：如何把文档合理的拆分成多个chunks，长度
3. 向量化：选择什么样的模型去向量化embedding
4. 向量数据库：选择什么样的向量数据库（milvus、faiss-轻量）
5. 选择什么样的检索方式来拿到精确的context（检索效率和准确率）：cosine相似度或者其它
6. context做ranking（排序）：常见的就是根据相似度分数进行排序（粗排）、也可以使用其它模型对用户问题和拿到的context重新计算相关性进行re-ranking重排（精排）
7. 用户question：是否增加用户的问题，问题太短导致意图不明确，问题太复杂需要拆解，问题包含歧义等。譬如：用户问题：“量子计算对密码学有什么影响？”，问题增强：1.明确“量子计算”和“密码学”的具体方面。2.检索相关文献，了解量子计算对密码学的具体影响。3.检索相关文献，了解量子计算对密码学的具体影响。
8. prompt模版设计：一个好的prompt模版对LLM的输出结果有很大的影响、可以更好的激发大模型的潜能
9. 大模型的选择：到底使用什么样的大模型：语言、设备方面、回答的质量方面、直接使用开源大模型还是需要微调
10. 大模型回复的后处理：直接使用大模型的内容回复还是做后处理（需求方面可能有回复的规范），可以加入某种检查机制检查是否符合输出规范
11. 最终关注哪些细节方面看你对整体流程的把控，去查看某个细节的输出，明显感受到其中的效果无法达到预期效果后去改善