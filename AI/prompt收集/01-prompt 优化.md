# 优化方向
- **Prompt 折叠 (Prompt Folding)**：一个分类器 Prompt 可以根据前一个查询动态生成一个更专业的 Prompt。
- **利用失败案例改进**：可以将导致 Prompt 失效的案例反馈给 LLM，让它帮助改进原 Prompt。由于 LLM 对自身「更了解」，这种方式非常有效。
- **扮演专家角色**：一个简单的元提示技巧是，让 LLM 扮演「Prompt 工程专家」，然后输入你自己的 Prompt，它会给出详细的改进建议。Harj 表示，可以持续迭代这个过程。
- **大模型优化，小模型执行**：Diana 提到，一些公司会用更强大的模型（如 Claude 3 Opus 或 GPT-4）进行元提示，优化得到一个高质量 Prompt 后，再将其用于更小、更快的模型。这对于需要低延迟的语音 AI 智能体尤为重要，以通过「图灵测试」的感觉。
- **「抱怨」机制**：YC 内部的一个创新方法是，在 LLM 的响应格式中增加一个「调试信息」(debug info) 参数，允许 LLM 向开发者「抱怨」输入信息模糊或不充分。这会形成一个待办事项列表，指导开发者改进 Prompt。

# 评估优化
尽管 Prompt 本身非常重要，但**评估 (Evals) 才是这些 AI 公司真正的「皇冠上的明珠」和数据资产**。ParaHelp 之所以愿意公开其 Prompt，部分原因在于他们认为，**没有评估，就无法理解 Prompt 为何这样设计，也难以对其进行改进**。