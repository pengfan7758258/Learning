[官方网址](https://docs.vllm.ai/en/latest/cli/serve.html?h=vllm+serve#cacheconfig)

启动命令：`vllm serve 模型路径`

- --tensor_parallel_size：tensor并行大小
- --pipeline_parallel_size：管道并行大小
原文中有这样的描述：
1. tensor_parallel_size管控GPU个数、pipeline_parallel_size管控node个数
譬如：如果有4台机器，每台机器有2块GPU，那么：
tensor_parallel_size=2，pipeline_parallel_size=4是设置的比较合适的，这其中主要是涉及的是底层的通信带宽。
2. 在单node中，如果只有奇数个GPU，那么请将tensor_parallel_size设置为1，pipeline_parallel_size设置为gpu个数。
3. 如果gpu节点之间没有NVlink连接，那么请将tensor_parallel_size设置为1，pipeline_parallel_size设置为gpu个数。


-  --gpu-memory-utilization：服务启动后gpu内存占用率，默认是0.9。包括模型、kv cache等。
- --max-model-len：模型生成的最大上下文长度。