1. 使用“Chain of thought”思维链探索所有可能的路径并且验证它生成的答案
2. 可以使用更少的上下文（context）获得结果
	- 训练阶段已经能让模型可以接受更少的prompt进行更深层次、更复杂的推理
	- 通过提供的question和少量的prompt，并通过自我对话的方式来解决问题
3. 强化学习是关键，RLHF做的越多，模型越准确
4. cot做的越多，性能（效果）越好，在推理时，获得更好的结果
	- 纠正错误
	- 尝试更多策略
	- 将问题分解更细
1. teaching verify，让模型学会自我校验
2. outputs via consensus voting（共识投票输出）
3. Abstract reasoning（抽象推理）：案例：给16个单词，让模型去给它分为4类，模型自己归纳总结，给出结果。简单说就是让模型从一堆数据中找出其中的规则![[01-o1简介-2.png]]

**版本：**
1. o1：推理复杂任务，支持函数调用（function calling）、图片输入（image input）
2. o1-mini：更快，更经济针对coding、math、science（成本更低、延迟更低）

gpt4o、o1-preview、o1、expert human比较：
（Competition Math数学竞赛、Code编码、Science Question科学问答）
![[01-o1简介-1.png]]

**input token、output token数：**
prompt token：输入token
completion token：生成token
resoning token：推理token
output token：输出token = completion token - resoning token
实际支付的是completion token的money，真实输出的是output token
所以o1一般是适用于复杂的任务