[åœ°å€](https://python.langchain.com/docs/tutorials/qa_chat_history/)

å¤šæ­¥éª¤æ£€ç´¢ï¼Œé•¿æœŸè®°å¿†

<mark style="background: #FFB86CA6;">chainæ–¹å¼å®ç°</mark>
```python
# æ¨¡å‹åˆå§‹åŒ–
from langchain.chat_models import init_chat_model
llm = init_chat_model(
Â  Â  model="gpt-4o-mini",
Â  Â  model_provider="openai"
)

# embeddingæ¨¡å‹ï¼Œè¿™ä¸ªéå¸¸å±é™©ï¼Œä¼šæš´éœ²openai_key
from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# web documents
import bs4
from langchain import hub
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from typing_extensions import List, TypedDict

  
# è¯»å–ç½‘é¡µåšå®¢å¹¶chunk
loader = WebBaseLoader(
Â  Â  web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
Â  Â  bs_kwargs=dict(
Â  Â  Â  Â  parse_only=bs4.SoupStrainer(
Â  Â  Â  Â  Â  Â  class_=("post-content", "post-title", "post-header")
Â  Â  Â  Â  )
Â  Â  ),
)

docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)

# vector store
from langchain_community.vectorstores import FAISS
vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)

# langgraphåˆ›å»ºgraph_builder,ä½¿ç”¨MessagesStateç®¡ç†State
from langgraph.graph import MessagesState, StateGraph
graph_builder = StateGraph(MessagesState)

# å°†æ£€ç´¢æ­¥éª¤ è½¬æ¢ä¸º å·¥å…·tool
from langchain_core.tools import tool

@tool(response_format="content_and_artifact")
def retrieve(query: str):
Â  Â  """æ£€ç´¢å’Œqueryç›¸å…³çš„ä¿¡æ¯."""
Â  Â  retrieved_docs = vector_store.similarity_search(query, k=2)
Â  Â  serialized = "\n\n".join(
Â  Â  Â  Â  (f"Source: {doc.metadata}\n" f"Content: {doc.page_content}")
Â  Â  Â  Â  for doc in retrieved_docs
Â  Â  )
Â  Â  return serialized, retrieved_docs

# æ„å»ºgraph
from langchain_core.messages import SystemMessage
from langgraph.prebuilt import ToolNode
# 1.ç”ŸæˆAImessageå¹¶å¸¦æœ‰æ£€ç´¢å·¥å…·è°ƒç”¨
def query_or_respond(state: MessagesState):
Â  Â  llm_with_tools = llm.bind_tools([retrieve])
Â  Â  response = llm_with_tools.invoke(state["messages"])
Â  Â  # MessagesStateå°†æ¶ˆæ¯é™„åŠ åˆ°çŠ¶æ€ï¼Œè€Œä¸æ˜¯è¦†ç›–
Â  Â  return {"messages": [response]}

# 2.retrievalçš„å·¥å…·Node
tools = ToolNode([retrieve])

# 3.ä½¿ç”¨æ£€ç´¢åˆ°çš„å†…å®¹ç”Ÿæˆå“åº”
def generate(state: MessagesState):
Â  Â  """ç”Ÿæˆç­”æ¡ˆ."""
Â  Â  # Get generated ToolMessages
Â  Â  recent_tool_messages = []
Â  Â  for message in reversed(state["messages"]): # æ‰¾å¯»æœ€è¿‘çš„çš„tool message
Â  Â  Â  Â  if message.type == "tool":
Â  Â  Â  Â  Â  Â  recent_tool_messages.append(message)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  break
Â  Â  tool_messages = recent_tool_messages[::-1]

Â  Â  # Format prompt
Â  Â  docs_content = "\n\n".join(doc.content for doc in tool_messages)
Â  Â  system_message_content = (
Â  Â  Â  Â  "You are an assistant for question-answering tasks. "
Â  Â  Â  Â  "Use the following pieces of retrieved context to answer "
Â  Â  Â  Â  "the question. If you don't know the answer, say that you "
Â  Â  Â  Â  "don't know. Use three sentences maximum and keep the "
Â  Â  Â  Â  "answer concise."
Â  Â  Â  Â  "\n\n"
Â  Â  Â  Â  f"{docs_content}"
Â  Â  )
Â  Â  
Â  Â  conversation_messages = [
Â  Â  Â  Â  message
Â  Â  Â  Â  for message in state["messages"]
Â  Â  Â  Â  if message.type in ("human", "system")
Â  Â  Â  Â  or (message.type == "ai" and not message.tool_calls)
Â  Â  ]

Â  Â  prompt = [SystemMessage(system_message_content)] + conversation_messages

Â  Â  # Run
Â  Â  response = llm.invoke(prompt)
Â  Â  return {"messages": [response]}

# ç¼–è¯‘graph
from langgraph.graph import END
from langgraph.prebuilt import tools_condition

graph_builder.add_node(query_or_respond)
graph_builder.add_node(tools)
graph_builder.add_node(generate)

graph_builder.set_entry_point("query_or_respond") # ä¸add_edge(START,query_or_respond)ä½œç”¨ä¸€æ ·

# å…è®¸query_or_respondå‡½æ•°short-circuitçŸ­è·¯ï¼š
# å¦‚æœå®ƒæ²¡æœ‰ç”Ÿæˆå·¥å…·è°ƒç”¨ï¼Œåˆ™ç›´æ¥å“åº”ç”¨æˆ·ï¼Œä½¿æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºèƒ½å¤Ÿæ”¯æŒå¯¹è¯ä½“éªŒ -- ä¾‹å¦‚ï¼Œå“åº”å¯èƒ½ä¸éœ€è¦æ£€ç´¢æ­¥éª¤çš„é€šç”¨é—®å€™è¯­â€œhiâ€
graph_builder.add_conditional_edges( # æ·»åŠ æ¡ä»¶è¾¹ï¼Œæ ¹æ®æ¡ä»¶å‡½æ•°çš„ç»“æœå†³å®šä¸‹ä¸€æ­¥çš„èµ°å‘
Â  Â  "query_or_respond", # è¿™æ˜¯æºèŠ‚ç‚¹ï¼Œè¡¨ç¤ºæ¡ä»¶è¾¹ä»è¯¥èŠ‚ç‚¹å‡ºå‘
Â  Â  tools_condition, # è¿™æ˜¯æ¡ä»¶å‡½æ•°ï¼Œç”¨äºå†³å®šä¸‹ä¸€æ­¥çš„èµ°å‘
Â  Â  {END: END, "tools": "tools"}, # è¿™æ˜¯ä¸€ä¸ªæ˜ å°„å­—å…¸ï¼Œè¡¨ç¤ºæ¡ä»¶å‡½æ•°çš„è¿”å›å€¼å¯¹åº”çš„ç›®æ ‡èŠ‚ç‚¹ - å¦‚æœ tools_condition è¿”å› ENDï¼Œåˆ™å›¾å°†ç»“æŸ; å¦‚æœ tools_condition è¿”å› "tools"ï¼Œåˆ™å›¾å°†è·³è½¬åˆ° tools èŠ‚ç‚¹

)
graph_builder.add_edge("tools", "generate")
graph_builder.add_edge("generate", END)
graph = graph_builder.compile()

# å¯è§†åŒ– control flow
from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))
```
![[09-RAG part 2.png]]
```python
# Test
input_message = "Hello"

for step in graph.stream(
Â  Â  {"messages": [{"role": "user", "content": input_message}]},
Â  Â  stream_mode="values",
):
Â  Â  step["messages"][-1].pretty_print()
"""
output:
========[1m Human Message [0m================
Hello
==========[1m Ai Message [0m================
Hello! How can I assist you today?
"""

input_message = "What is Task Decomposition?"

for step in graph.stream(
Â  Â  {"messages": [{"role": "user", "content": input_message}]},
Â  Â  stream_mode="values",
):
Â  Â  step["messages"][-1].pretty_print()
"""
output:
=========== Human Message ================
What is Task Decomposition?
============ Ai Message =========
Tool Calls: 
retrieve (call_UEMguofyc7ktfXuyGGsf2E38) 
Call ID: call_UEMguofyc7ktfXuyGGsf2E38 
Args: 
query: Task Decomposition
============== Tool Message ================
Name: retrieve
......
============== Ai Message ===================
Task Decomposition is the process of breaking down a ...
"""

# Memory persistence
from langgraph.checkpoint.memory import MemorySaver
graph = graph_builder.compile(checkpointer=MemorySaver())
# ä¸ºå½“å‰çº¿ç¨‹æŒ‡å®šä¸€ä¸ªid
config = {"configurable": {"thread_id": "abc123"}}
# åé¢æœ‰éƒ¨åˆ†æ²¡å¤åˆ¶ï¼Œå°±æ˜¯æŠŠconfigå‚æ•°ç»™graph.streamå½“å‚æ•°å°±èƒ½è®°ä½å†å²èŠå¤©
```

<mark style="background: #FFB86CA6;">agentæ–¹å¼å®ç°</mark>
```python
from langgraph.prebuilt import create_react_agent

agent_executor = create_react_agent(llm, [retrieve], checkpointer=MemorySaver())

display(Image(agent_executor.get_graph().draw_mermaid_png()))
"""
è¿™é‡Œçš„å·¥å…·è°ƒç”¨ä¸æ˜¯ç»“æŸè¿è¡Œçš„æœ€ç»ˆç”Ÿæˆæ­¥éª¤è€Œæ˜¯å¾ªç¯å›åˆ°åŸå§‹LLMè°ƒç”¨ã€‚
ç„¶å,æ¨¡å‹å¯ä»¥ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜æˆ–ç”Ÿæˆå¦ä¸€ä¸ªå·¥å…·è°ƒç”¨ä»¥è·å–æ›´å¤šä¿¡æ¯
"""
```
![[09-RAG part 2 1.png]]
```python
config = {"configurable": {"thread_id": "def237"}}

  
input_message = (
Â  Â  "What is the standard method for Task Decomposition?\n\n"
Â  Â  "Once you get the answer, look up common extensions of that method."
)

# æ‰¾åˆ°ä¸€ä¸ªé—®é¢˜ç­”æ¡ˆåå†æ¬¡è°ƒç”¨toolså»æ£€ç´¢ç›¸å…³ä¿¡æ¯
for event in agent_executor.stream(
Â  Â  {"messages": [{"role": "user", "content": input_message}]},
Â  Â  stream_mode="values",
Â  Â  config=config,
):
Â  Â  event["messages"][-1].pretty_print()
"""
============== Human Message ================
What is the standard method for Task Decomposition? Once you get the answer, look up common extensions of that method.
============== Ai Message ================
Tool Calls: 
retrieve (call_ESh3TDYMa50F9rPl3oUNbREi) 
Call ID: call_ESh3TDYMa50F9rPl3oUNbREi 
Args: 
query: standard method for Task Decomposition
============== Tool Message ================
Name: retrieve
....
============== Ai Message ================
Tool Calls: 
retrieve (call_X1kbJbHnswkj8TV8YjDEy3Hc) 
Call ID: call_X1kbJbHnswkj8TV8YjDEy3Hc 
Args: 
query: extensions of Task Decomposition method 
============== Tool Message ================
Name: retrieve
...
============== Ai Message ================
### Standard Method for Task Decomposition
...
### Common Extensions of Task Decomposition
...
"""
```
