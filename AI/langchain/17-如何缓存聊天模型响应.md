- [地址](https://python.langchain.com/docs/how_to/chat_model_caching/)
- 如果您经常多次请求相同的完成，它可以通过减少对LLM提供商进行的 API 调用次数来节省资金
- 相同请求可以加速响应，因为是直接从内存中取出进行响应

<mark style="background: #FFB86CA6;">基本配置：</mark>
```python
from langchain.chat_models import init_chat_model  
# 加载模型
llm = init_chat_model("gpt-4o-mini", model_provider="openai")
```

<mark style="background: #FFB86CA6;">In Memory Cache:</mark>
- 这是一个临时缓存，用于将模型调用存储在内存中。当您的环境重新启动时，它将被擦除，并且不会在进程之间共享。
```python
%%time
from langchain_core.globals import set_llm_cache
from langchain_core.caches import InMemoryCache

set_llm_cache(InMemoryCache())

# 第一次运行没有缓存，花费时间略长
llm.invoke("Tell me a joke")

"""
------------ output --------------
CPU times: total: 78.1 ms
Wall time: 1.69 s

AIMessage(content='Why don’t scient...)
"""

%%time
# 第二次运行，飞快
llm.invoke("Tell me a joke")
"""
------------ output --------------
CPU times: total: 0 ns
Wall time: 998 μs

AIMessage(content='Why don’t scient...)
"""

%%time
# 换个问题试试
llm.invoke("告诉我三国时期有几个国家")
"""
------------ output --------------
CPU times: total: 0 ns
Wall time: 1.97 s

AIMessage(content='三国时期主要有三个国家...)
"""
```

<mark style="background: #FFB86CA6;">SQLite Cache：</mark>
- 此缓存实现使用 `SQLite` 数据库来存储响应，项目即使重启也可以从缓存毫秒级获得数据
```python
# SQLite cache
from langchain_core.globals import set_llm_cache
from langchain_community.cache import SQLiteCache

set_llm_cache(SQLiteCache(database_path=".langchain.db"))

%%time
# 第一次运行没有缓存，花费时间略长
llm.invoke("Tell me a joke")
"""
------------ output --------------
CPU times: total: 78.1 ms
Wall time: 1.69 s

AIMessage(content='Why don’t scient...)
"""

%%time
# 第二次运行，飞快
llm.invoke("Tell me a joke")
"""
------------ output --------------
CPU times: total: 0 ns
Wall time: 998 μs

AIMessage(content='Why don’t scient...)
"""

%%time
# 换个问题试试
llm.invoke("告诉我三国时期有几个国家")
"""
------------ output --------------
CPU times: total: 0 ns
Wall time: 1.97 s

AIMessage(content='三国时期主要有三个国家...)
"""
```