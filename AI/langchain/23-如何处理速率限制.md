- [地址](https://python.langchain.com/docs/how_to/chat_model_rate_limiting/)
- 发出的请求过多，受到模型提供程序 API 的速率限制
	- Langchain 带有一个内置的内存速率限制器。此速率限制器是线程安全的，可以由同一进程中的多个线程共享。
	- 提供的速率限制器只能限制每单位时间的请求数。如果您还需要根据请求的大小进行限制，这将无济于事。

```python
from langchain_core.rate_limiters import InMemoryRateLimiter

# 初始化请求速率器
rate_limiter = InMemoryRateLimiter(
    requests_per_second=0.1,  # 每10秒发起一次请求!
    check_every_n_seconds=0.1,  # 每100毫秒唤醒一次，检查是否允许发出请求
    max_bucket_size=10,  # 控制最大bucket size

)

from langchain.chat_models import init_chat_model

# 加载模型
llm = init_chat_model(
    model="gpt-4o-mini",
    model_provider="openai",
    rate_limiter=rate_limiter
)

import time
for _ in range(5):
    tic = time.time()
    llm.invoke("hello")
    toc = time.time()
    print(toc - tic)
"""
-------- output ---------
12.218457221984863
9.221540689468384
10.041089057922363
9.84594178199768
10.085010528564453
"""
```