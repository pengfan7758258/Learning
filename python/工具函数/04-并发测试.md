```python
import asyncio
import httpx
import time

# LLM 接口地址（替换为你自己的流式接口地址）
LLM_API_URL = "http://127.0.0.1:5000/test/stream"

# 模拟请求 payload（替换为你的实际 payload）
REQUEST_PAYLOAD = {
	"字段一": "值",
}

CONCURRENCY = 10

# 存储每个请求第一个token和完成的时间
first_token_times = []
completed_times = []

async def fetch_streaming_response(client, index, start_time):
	first_token_received = False

	async with client.stream("POST", LLM_API_URL, json=REQUEST_PAYLOAD, timeout=60.0) as response:
		async for line in response.aiter_lines():
			if line.strip():
				now = time.time()
				if not first_token_received:
					first_token_received = True
					delta = now - start_time
					first_token_times.append(delta)
				# 可选：打印每行数据
				print(f"[{index}] {line}")
		completed_times.append(time.time() - start_time)


async def main():
	print(f"开始测试，当前并发量为:{CONCURRENCY}")

	start_time = time.time()
	async with httpx.AsyncClient() as client:
		tasks = [
			fetch_streaming_response(client, i, start_time)
			for i in range(CONCURRENCY)
		]
		await asyncio.gather(*tasks)
	
	# 输出统计结果
	max_first_token_time = max(first_token_times)
	max_complete_time = max(completed_times)
	
	print(f"所有请求收到第一个token的最长耗时：{max_first_token_time:.3f} 秒")
	print(f"所有请求完全结束的最长耗时：{max_complete_time:.3f} 秒")

if __name__ == "__main__":
	asyncio.run(main())
```